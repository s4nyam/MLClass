# -*- coding: utf-8 -*-
"""Assignment 4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14obgE5tle-oqssi6XBk9AEvD0QSmWY1M

Implement a handwritten digit classifier using a neural network. 
You can use available routines in Scikit-Learn or Tensorflow to program the neural network. 

Fix an appropriate architecture and experiment with different batch sizes. 

Generate new training images by flipping the white and black pixels. That means, the foreground which was originally black will now become white and the background which was originally white will now become black. 
Now prepare batches which have equal number of original and the corresponding flipped images. 

Plot the training error vs update step for every experiment.

# Part 1 - **Neural Network Implementation**
"""

import tensorflow as tf
mnist = tf.keras.datasets.mnist # 28*28 images of digits 0-9

(x_train, y_train), (x_test,y_test) = mnist.load_data()

# Visualization
import matplotlib.pyplot as plt
# print(x_train[0])
plt.imshow(x_train[0], cmap=plt.cm.binary) # Printing in 2 colors only
plt.show()

(x_train, y_train), (x_test,y_test) = mnist.load_data()
x_train = tf.keras.utils.normalize(x_train,axis=1)
x_test = tf.keras.utils.normalize(x_test,axis=1)
# print(x_train[0])
plt.imshow(x_train[0], cmap=plt.cm.binary)
plt.show()

# The image is diminished because we have normalized the datapoints between 0 to 1
print(x_train[0]) # Scaled between 0 and 1

# (x_train, y_train), (x_test,y_test) = mnist.load_data()
# x_train = tf.keras.utils.normalize(x_train,axis=1)
# x_test = tf.keras.utils.normalize(x_train,axis=1)
# print(x_train[0])
# Build the model
model = tf.keras.models.Sequential() # Feed forward model

model.add(tf.keras.layers.Flatten()) # used for input layer to flatten it
# 128 neurons and activation function to be rectified linear
# Layer 1 
model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))
# Layer 2 
model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))
# Output layer with 10 classifications
# Softmax for probablity distribution
model.add(tf.keras.layers.Dense(10, activation=tf.nn.softmax))

# Training
# Use thie crossentropy loss function when there are two or more label classes. 
# It expect labels to be provided in a one_hot representation. 
# If you want to provide labels as integers, please use 
# SparseCategoricalCrossentropy loss. There should be # classes floating point 
# values per feature.
model.compile(optimizer="SGD",loss='sparse_categorical_crossentropy',metrics=['accuracy'])
model.fit(x_train, y_train, epochs=3) # Passes
# you can’t pass the entire dataset into the neural net at once. 
# So, you divide dataset into Number of Batches or sets or parts.
# Just like you divide a big article into multiple sets/batches/parts like Introduction, 
# Gradient descent, Epoch, Batch size and Iterations which makes it easy to read the entire
# article for the reader and understand it. 
# For xample: We can divide the dataset of 2000 examples into 
# batches of 500 then it will take 4 iterations to complete 1 epoch.

val_loss,val_accuracy=model.evaluate(x_test,y_test)
# loss: 0.2888 - accuracy: 0.9165

# # You can save and load saved models as well using keras.

# model.save('new_model')
# load_model=tf.keras.models.load_model('new_model')

Prediction = model.predict([x_test])
print(Prediction) # Prints probablity distributions

import numpy as np
np.argmax(Prediction[0])

plt.imshow(x_test[0])

Prediction[0]

# On checking this array, we get different probablities of the numerics
# Probablity of getting 1 -> 5.02
# Probablity of getting 2 -> 2.67
# .
# .
# .
# Probablity of getting 7 -> 9.98 (which is max)

# Lets change batch size!!~!!



"""Batch Size effect on training set"""

# Changing batch size = 30 for training 
# Loss - 0.28395113348960876 Accuracy - 0.9204999804496765
(x_train, y_train), (x_test,y_test) = mnist.load_data()
x_train = tf.keras.utils.normalize(x_train,axis=1)
x_test = tf.keras.utils.normalize(x_test,axis=1)
# print(x_train[0])
plt.imshow(x_train[0], cmap=plt.cm.binary)
plt.show()

# The image is diminished because we have normalized the datapoints between 0 to 1
# print(x_train[0]) # Scaled between 0 and 1
# (x_train, y_train), (x_test,y_test) = mnist.load_data()
# x_train = tf.keras.utils.normalize(x_train,axis=1)
# x_test = tf.keras.utils.normalize(x_train,axis=1)
# print(x_train[0])
# Build the model
model = tf.keras.models.Sequential() # Feed forward model
model.add(tf.keras.layers.Flatten()) # used for input layer to flatten it
# 128 neurons and activation function to be rectified linear
# Layer 1 
model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))
# Layer 2 
model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))
# Output layer with 10 classifications
# Softmax for probablity distribution
model.add(tf.keras.layers.Dense(10, activation=tf.nn.softmax))

# Training
# Use thie crossentropy loss function when there are two or more label classes. 
# It expect labels to be provided in a one_hot representation. 
# If you want to provide labels as integers, please use 
# SparseCategoricalCrossentropy loss. There should be # classes floating point 
# values per feature.
model.compile(optimizer="SGD",loss='sparse_categorical_crossentropy',metrics=['accuracy'])
model.fit(x_train, y_train, batch_size=30, epochs=3) # Passes
val_loss_30,val_accuracy_30=model.evaluate(x_test,y_test)
print(val_loss_30,val_accuracy_30)

# Changing batch size = 50 for training 
# Loss - 0.33094361424446106 Accuracy - 0.909500002861023
(x_train, y_train), (x_test,y_test) = mnist.load_data()
x_train = tf.keras.utils.normalize(x_train,axis=1)
x_test = tf.keras.utils.normalize(x_test,axis=1)
# print(x_train[0])
plt.imshow(x_train[0], cmap=plt.cm.binary)
plt.show()

# The image is diminished because we have normalized the datapoints between 0 to 1
# print(x_train[0]) # Scaled between 0 and 1
# (x_train, y_train), (x_test,y_test) = mnist.load_data()
# x_train = tf.keras.utils.normalize(x_train,axis=1)
# x_test = tf.keras.utils.normalize(x_train,axis=1)
# print(x_train[0])
# Build the model
model = tf.keras.models.Sequential() # Feed forward model
model.add(tf.keras.layers.Flatten()) # used for input layer to flatten it
# 128 neurons and activation function to be rectified linear
# Layer 1 
model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))
# Layer 2 
model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))
# Output layer with 10 classifications
# Softmax for probablity distribution
model.add(tf.keras.layers.Dense(10, activation=tf.nn.softmax))

# Training
# Use thie crossentropy loss function when there are two or more label classes. 
# It expect labels to be provided in a one_hot representation. 
# If you want to provide labels as integers, please use 
# SparseCategoricalCrossentropy loss. There should be # classes floating point 
# values per feature.
model.compile(optimizer="SGD",loss='sparse_categorical_crossentropy',metrics=['accuracy'])
model.fit(x_train, y_train, batch_size=50, epochs=3) # Passes
val_loss_50,val_accuracy_50=model.evaluate(x_test,y_test)
print(val_loss_50,val_accuracy_50)

# Changing batch size = 100 for training 
# Loss - 0.47130557894706726, Accuracy - 0.8769999742507935
(x_train, y_train), (x_test,y_test) = mnist.load_data()
x_train = tf.keras.utils.normalize(x_train,axis=1)
x_test = tf.keras.utils.normalize(x_test,axis=1)
# print(x_train[0])
plt.imshow(x_train[0], cmap=plt.cm.binary)
plt.show()

# The image is diminished because we have normalized the datapoints between 0 to 1
# print(x_train[0]) # Scaled between 0 and 1
# (x_train, y_train), (x_test,y_test) = mnist.load_data()
# x_train = tf.keras.utils.normalize(x_train,axis=1)
# x_test = tf.keras.utils.normalize(x_train,axis=1)
# print(x_train[0])
# Build the model
model = tf.keras.models.Sequential() # Feed forward model
model.add(tf.keras.layers.Flatten()) # used for input layer to flatten it
# 128 neurons and activation function to be rectified linear
# Layer 1 
model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))
# Layer 2 
model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))
# Output layer with 10 classifications
# Softmax for probablity distribution
model.add(tf.keras.layers.Dense(10, activation=tf.nn.softmax))

# Training
# Use thie crossentropy loss function when there are two or more label classes. 
# It expect labels to be provided in a one_hot representation. 
# If you want to provide labels as integers, please use 
# SparseCategoricalCrossentropy loss. There should be # classes floating point 
# values per feature.
model.compile(optimizer="SGD",loss='sparse_categorical_crossentropy',metrics=['accuracy'])
model.fit(x_train, y_train, batch_size=100, epochs=3) # Passes
val_loss_100,val_accuracy_100=model.evaluate(x_test,y_test)
print(val_loss_100,val_accuracy_100)

"""1.  batch size, 30 Loss - 0.28395113348960876 Accuracy - 0.9204999804496765

2.  batch size, 50 Loss - 0.33094361424446106 Accuracy - 0.909500002861023

3.  batch size, 100 Loss - 0.47130557894706726, Accuracy - 0.8769999742507935

As observed, Loss is increasing as we increasing batch size and Accuracy is decreasing
"""

# Lets try for larger batch size now
# Changing batch size = 1000 for training 
# Loss - 2.151988983154297 Accuracy - 0.44769999384880066
(x_train, y_train), (x_test,y_test) = mnist.load_data()
x_train = tf.keras.utils.normalize(x_train,axis=1)
x_test = tf.keras.utils.normalize(x_test,axis=1)
# print(x_train[0])
plt.imshow(x_train[0], cmap=plt.cm.binary)
plt.show()

# The image is diminished because we have normalized the datapoints between 0 to 1
# print(x_train[0]) # Scaled between 0 and 1
# (x_train, y_train), (x_test,y_test) = mnist.load_data()
# x_train = tf.keras.utils.normalize(x_train,axis=1)
# x_test = tf.keras.utils.normalize(x_train,axis=1)
# print(x_train[0])
# Build the model
model = tf.keras.models.Sequential() # Feed forward model
model.add(tf.keras.layers.Flatten()) # used for input layer to flatten it
# 128 neurons and activation function to be rectified linear
# Layer 1 
model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))
# Layer 2 
model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))
# Output layer with 10 classifications
# Softmax for probablity distribution
model.add(tf.keras.layers.Dense(10, activation=tf.nn.softmax))

# Training
# Use thie crossentropy loss function when there are two or more label classes. 
# It expect labels to be provided in a one_hot representation. 
# If you want to provide labels as integers, please use 
# SparseCategoricalCrossentropy loss. There should be # classes floating point 
# values per feature.
model.compile(optimizer="SGD",loss='sparse_categorical_crossentropy',metrics=['accuracy'])
model.fit(x_train, y_train, batch_size=1000, epochs=3) # Passes
val_loss_1000,val_accuracy_1000=model.evaluate(x_test,y_test)
print(val_loss_1000,val_accuracy_1000)

"""Hence we can say that as we incrrease the batch size we get poor results.

for batch size 1000, Loss - 2.151988983154297 Accuracy - 0.44769999384880066

Batch Size effect in test set
"""

# impact the batch_size has when evaluating the model on a test set
# it is relevant for training as it determines the number of samples to be fed
# to the network before computing the next gradient.
# Changing batch size = 30 for training 
# Loss - 0.28395113348960876 Accuracy - 0.9204999804496765
(x_train, y_train), (x_test,y_test) = mnist.load_data()
x_train = tf.keras.utils.normalize(x_train,axis=1)
x_test = tf.keras.utils.normalize(x_test,axis=1)
# print(x_train[0])
plt.imshow(x_train[0], cmap=plt.cm.binary)
plt.show()

# The image is diminished because we have normalized the datapoints between 0 to 1
# print(x_train[0]) # Scaled between 0 and 1
# (x_train, y_train), (x_test,y_test) = mnist.load_data()
# x_train = tf.keras.utils.normalize(x_train,axis=1)
# x_test = tf.keras.utils.normalize(x_train,axis=1)
# print(x_train[0])
# Build the model
model = tf.keras.models.Sequential() # Feed forward model
model.add(tf.keras.layers.Flatten()) # used for input layer to flatten it
# 128 neurons and activation function to be rectified linear
# Layer 1 
model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))
# Layer 2 
model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))
# Output layer with 10 classifications
# Softmax for probablity distribution
model.add(tf.keras.layers.Dense(10, activation=tf.nn.softmax))

# Training
# Use thie crossentropy loss function when there are two or more label classes. 
# It expect labels to be provided in a one_hot representation. 
# If you want to provide labels as integers, please use 
# SparseCategoricalCrossentropy loss. There should be # classes floating point 
# values per feature.
model.compile(optimizer="SGD",loss='sparse_categorical_crossentropy',metrics=['accuracy'])
model.fit(x_train, y_train, batch_size=30, epochs=3) # Passes
val_loss_30,val_accuracy_30=model.evaluate(x_test,y_test)
print(val_loss_30,val_accuracy_30)


val_loss2,val_accuracy2=model.evaluate(x_test, y_test, batch_size = 10)
val_loss3,val_accuracy3=model.evaluate(x_test, y_test, batch_size = 20)
val_loss4,val_accuracy4=model.evaluate(x_test, y_test, batch_size = 30)
val_loss5,val_accuracy5=model.evaluate(x_test, y_test, batch_size = 40)
val_loss6,val_accuracy6=model.evaluate(x_test, y_test, batch_size = 50)
print(val_loss2,val_accuracy2)
print(val_loss3,val_accuracy3)
print(val_loss4,val_accuracy4)
print(val_loss5,val_accuracy5)
print(val_loss6,val_accuracy6)

"""In testing, loss and accuracy remains same even changing multiple"""



"""# Part 2 - New Images by **inverting black and white pixels**"""

# Flipping the colors that is BW to WB
# In order to invert the pixels we just have to subtract 255
import numpy as np

(x_train, y_train), (x_test,y_test) = mnist.load_data()
x_train = tf.keras.utils.normalize(x_train,axis=1)
x_test = tf.keras.utils.normalize(x_test,axis=1)
# print(x_train[0])
plt.imshow(x_train[3], cmap=plt.cm.binary)
plt.show()

# import pandas as pd
# import numpy as np
# import matplotlib.pyplot as plt
# data=pd.read_csv("https://raw.githubusercontent.com/sjwhitworth/golearn/master/examples/datasets/mnist_train.csv")
# data.values[:, 1:] = np.abs(data.values[:, 1:] - 255)  # color inversion
# x=data.values[-1, 1:]  # plot latest row in the dataset
# plt.imshow(x.reshape(28, 28), cmap='gray')
# plt.show()

# Displaying the inverted image

(x_train, y_train), (x_test,y_test) = mnist.load_data()
x_train = tf.keras.utils.normalize(x_train,axis=1)
x_test = tf.keras.utils.normalize(x_test,axis=1)
# print(x_train[0])
x_train_inverted = np.abs(x_train - 255)
plt.imshow(x_train_inverted[3], cmap=plt.cm.binary)
plt.show()

plt.imshow(x_train[60], cmap=plt.cm.binary)

plt.imshow(x_train_inverted[60], cmap=plt.cm.binary)

# Since underlying number of the image remains same, we do not require to change the y labels.

# Performing the training

# Build the model
model = tf.keras.models.Sequential() # Feed forward model

model.add(tf.keras.layers.Flatten()) # used for input layer to flatten it
# 128 neurons and activation function to be rectified linear
# Layer 1 
model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))
# Layer 2 
model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))
# Output layer with 10 classifications
# Softmax for probablity distribution
model.add(tf.keras.layers.Dense(10, activation=tf.nn.softmax))

# Training
# Use thie crossentropy loss function when there are two or more label classes. 
# It expect labels to be provided in a one_hot representation. 
# If you want to provide labels as integers, please use 
# SparseCategoricalCrossentropy loss. There should be # classes floating point 
# values per feature.
model.compile(optimizer="SGD",loss='sparse_categorical_crossentropy',metrics=['accuracy'])
model.fit(x_train_inverted, y_train, epochs=3) # Passes
# you can’t pass the entire dataset into the neural net at once. 
# So, you divide dataset into Number of Batches or sets or parts.
# Just like you divide a big article into multiple sets/batches/parts like Introduction, 
# Gradient descent, Epoch, Batch size and Iterations which makes it easy to read the entire
# article for the reader and understand it. 
# For xample: We can divide the dataset of 2000 examples into 
# batches of 500 then it will take 4 iterations to complete 1 epoch.

# Performing the test on inverted training iamges
# Testing images are not pixel flipped or inverted.
val_loss,val_accuracy=model.evaluate(x_test,y_test) 

print(val_loss,val_accuracy)

Prediction = model.predict([x_test])
print(Prediction) # Prints probablity distributions

import numpy as np
np.argmax(Prediction[3])

"""The flipping of images with their colors from B to W or W to b has changed our results drastically. We are getting a large amount of loss as well as negligible accuracy in our model. Loss - 225.17845153808594, Accuracy - 0.07609999924898148"""

# Let us flip the pixels of test images as well. 
plt.imshow(x_test[3], cmap=plt.cm.binary)

x_test_inverted = np.abs(x_test-255)
plt.imshow(x_test_inverted[3], cmap=plt.cm.binary)

model = tf.keras.models.Sequential() # Feed forward model

model.add(tf.keras.layers.Flatten()) 
# Layer 1 
model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))
# Layer 2 
model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))

model.add(tf.keras.layers.Dense(10, activation=tf.nn.softmax))

model.compile(optimizer="SGD",loss='sparse_categorical_crossentropy',metrics=['accuracy'])
model.fit(x_train_inverted, y_train, batch_size=20, epochs=3) # Passes

val_loss,val_accuracy=model.evaluate(x_test_inverted,y_test) 

print(val_loss,val_accuracy)

Prediction = model.predict([x_test_inverted])
# print(Prediction) # Prints probablity distributions
import numpy as np
np.argmax(Prediction[3])

